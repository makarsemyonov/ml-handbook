{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258173dd",
   "metadata": {},
   "source": [
    "# Конспект по машинному обучению для подготовки к собеседованиям\n",
    "Уровень: **Intern DS / ML**\n",
    "\n",
    "## 1. Основные понятия\n",
    "### 1.1. Задача обучения с учителем\n",
    "\n",
    "Пусть дано множество $X$ и множество $Y$, и существует целевая функция $y^* = X \\rightarrow Y$, значения которой $y_i = y^*(x_i), y_i \\in Y, x_i \\in X$ известны только на конечном подмножестве $ (x_1, x_2, ..., x_i) \\in X$. Совокупность объектов $X^i = (x_1, x_2, ..., x_i) \\in X$ называется выборкой.\n",
    "\n",
    "Задача обучения с учителем заключается в том, чтобы по выборке $X^l$ восстановить зависимость $y^*$, то есть построить решающую функцию $a: X \\rightarrow Y$ , которая приближала бы целевую функцию, причём\n",
    "не только на объектах обучающей выборки, но и на всём множестве $X$.\n",
    "\n",
    "Неформально это можно понимать так:\n",
    "\n",
    "У нас есть некая неизвестная функция (таргет), и мы знаем ее значения только на ограниченном количестве входных данных. Задача обучения с учителем заключается в том, найти такую функцию, которая бы приближала максимально точно таргет на всем множестве входных данных.\n",
    "\n",
    "### 1.2. Объекты и признаки\n",
    "\n",
    "Признаком $f$ объекта $x$  называется отображение $f: X \\rightarrow D_f$, где $D_f$ - множество допустимых значений признака. То есть признак - результат измерения некоторой характеристики объекта.\n",
    "\n",
    "В зависимости от природы множества $D_f$ признаки делятся на несколько типов:\n",
    "- Если $D_f = \\{0, 1\\}$, то $f$ бинарный признак\n",
    "- Если $D_f$ это конечное множество, то $f$ - номинальный признак\n",
    "- Если $D_f$ это конечное упорядоченное множество, то $f$ - порядковый признак\n",
    "- Если $D_f = R$, то $f$ - количественный признак\n",
    "\n",
    "### 1.3. Основные типы задач обучения с учителем\n",
    "\n",
    "В зависимости от природы множества $Y$ задачи обучения с учителем делятся на следующие типы:\n",
    "\n",
    "- Если множество $Y = \\{1, 2, ..., M\\}$, то задача называется задачей классификации на $M$ непересекающихся классов\n",
    "- Если $Y = \\{0, 1\\}^M$ - задача классификации на $M$ пересекающихся классов\n",
    "- Если $Y = R$ - задача регрессии\n",
    "\n",
    "### 1.4. Функция потерь и функционал качества\n",
    "\n",
    "Функция потерь, она же loss function, это неотрицательная функция $L(a, x)$, характеризующая величину ошибки решающей функции $a$, приближающей целевую функцию $y^*$ на объекте $x$. Если $L(a, x) = 0$, то ответ $a(x)$ называется корректным.\n",
    "\n",
    "Функционалом качества или эмпирическим риском называется среднее арифметическое функции потерь на выборке $X^l$: $$Q(a, X^l) = \\frac{1}{l}\\sum_{i=0}^{i=l}{L(a, x_i)}$$\n",
    "\n",
    "Истинным риском назвается математическое ожидание функции потерь на всем множесве $X$.\n",
    "\n",
    "\n",
    "### 1.5 Понятие модели\n",
    "\n",
    "Моделью называется параметрическое семейство отображений $A = \\{a(x, \\theta) | \\theta \\in \\Theta\\}$, где $a: X × \\Theta \\rightarrow Y$ некоторая фиксированная функция, $\\Theta$ множество допустимых значений параметра $\\theta$, называемое пространством параметров или пространством поиска.\n",
    "\n",
    "Неформально это можно понимать так:\n",
    "\n",
    "Модель это множество функций одинакового типа, но отличающихся каким-то параметром. Например множество линейных функций вида $y = ax + b$. Параметрами здесь являются $a$ и $b$, а моделью все возможные линейные функции. То есть можно можно записать данную модель так:\n",
    "$A = \\{a(x, (a, b)) | a \\in R, b \\in R \\}$. $\\theta$ - пара чисел $(a, b)$, $\\Theta$ - множество всех возможных значений пары чисел $(a, b)$.\n",
    "\n",
    "\n",
    "### 1.6 Понятие обучения\n",
    "\n",
    "Обучением модели называется процесс подбора параметра $\\theta$ функции $a$, принадлежащей модели $A$, по выборке $X^l$. \n",
    "\n",
    "Методом обучения называется отображение $\\mu = (X × Y)^l \\rightarrow A$, которое произвольной конечной выборке $X^l$ ставит в соответствие функцию $a \\in A$.\n",
    "\n",
    "Классический метод обучения, называемый минимизацией эмпирического риска, empirical risk minimization ERM, заключается в том, чтобы найти в заданной модели $A$ функцию $a$, доставляющую минимальное значение функционалу качества на заданной обучающей выборке $X^l$: $$\\hat\\theta = \\argmin_{a \\in A}Q(a, X^l)$$\n",
    "\n",
    "Неформально говоря:\n",
    "\n",
    "Мы предполагаем, что целевая функция $y^*$ принадлежит некоторому классу функций (модели), но конкретные параметры этой функции нам неизвестны. Обучение — это процесс выбора параметров модели по выборке так, чтобы функция с этими параметрами минимизировала ошибку на данных. Полученная функция и становится решающей функцией $a$.\n",
    "\n",
    "Например, мы предполагаем что таргет это линейная функция, то есть принадлежит линейной модели и имеет вид $y^*= ax+b$. Обучением будет подбор параметром $(a, b)$ по выборке так, чтобы минимизировать ошибку.\n",
    "\n",
    "### 1.7 Проблемы обучения - underfitting и overfitting\n",
    "\n",
    "Зачастую при обучении модели возникают две основные проблемы:\n",
    "\n",
    "- Недообучиение (underfitting) - модель слишком простая и не способна уловить зависимость в данных.\n",
    "В результате ошибки как на обучающей выборке, так и на новых данных остаются большими.\n",
    "- Переобучение (overfitting) — модель слишком сложная и запоминает все детали обучающей выборки, включая шум.\n",
    "В результате ошибка на обучающих данных низкая, но на новых данных значительно выше.\n",
    "\n",
    "Хорошая модель это та, которая улавливает закономерности и не запоминает шум.\n",
    "\n",
    "### 1.8 Проблемы обучения - bias-variance tradeoff\n",
    "\n",
    "Для того чтобы модель не переобучалась, но и не недообучалась необходимо найти баланс в сложности модели. Такой баланс называют bias-variance tradeoff. \n",
    "\n",
    "- bias (смещение) - систематическая ошибка модели\n",
    "- variance (разброс) - чувствительность модели к шуму в данных \n",
    "\n",
    "Если модель слишком простая, у неё высокий bias и низкий variance.\n",
    "Если модель слишком сложная, у неё низкий bias, но высокий variance.\n",
    "\n",
    "Пример: пусть целевая функция квадратичная.\n",
    "\n",
    "- Линейная модель будет слишком простой: её предсказания систематически ошибочны (высокий bias), но она меньше реагирует на шум (низкий variance).\n",
    "- Модель с кубической функцией будет слишком гибкой: она может запоминать шум и детали обучающей выборки (высокий variance), хотя bias будет низким.\n",
    "\n",
    "### 1.9 Регуляризация\n",
    "\n",
    "Регуляризация - это способ ограничить сложность модели, чтобы уменьшить переобучение. В классическом ERM добавляется штраф за слишком большие или слишком сложные параметры модели:\n",
    "$$\\hat\\theta = \\argmin_{a \\in A}[Q(a, X^l) + \\lambda\\Omega(\\theta)]$$\n",
    "- $Q(a, X^l)$ - эмпирический риск\n",
    "- $\\Omega$ - функция регуляризации\n",
    "- $\\lambda$ - коэффициент регуляризации, задающий степень ограничения\n",
    "\n",
    "Самые популярные виды регуляризации:\n",
    "- L2 (Ridge): уменьшает все веса, стабилизирует модель\n",
    "- L1 (Lasso): выполняет отбор признаков, обнуляя некоторые веса"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
