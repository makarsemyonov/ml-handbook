{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258173dd",
   "metadata": {},
   "source": [
    "# Конспект по машинному обучению для подготовки к собеседованиям\n",
    "Уровень: **Intern DS / ML**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Основные понятия\n",
    "### 1.1. Задача обучения с учителем\n",
    "\n",
    "Пусть дано множество $X$ и множество $Y$, и существует целевая функция $y^* = X \\rightarrow Y$, значения которой $y_i = y^*(x_i), y_i \\in Y, x_i \\in X$ известны только на конечном подмножестве $ (x_1, x_2, ..., x_i) \\in X$. Совокупность объектов $X^i = (x_1, x_2, ..., x_i) \\in X$ называется выборкой.\n",
    "\n",
    "Задача обучения с учителем заключается в том, чтобы по выборке $X^l$ восстановить зависимость $y^*$, то есть построить решающую функцию $a: X \\rightarrow Y$ , которая приближала бы целевую функцию, причём\n",
    "не только на объектах обучающей выборки, но и на всём множестве $X$.\n",
    "\n",
    "Неформально это можно понимать так:\n",
    "\n",
    "У нас есть некая неизвестная функция (таргет), и мы знаем ее значения только на ограниченном количестве входных данных. Задача обучения с учителем заключается в том, найти такую функцию, которая бы приближала максимально точно таргет на всем множестве входных данных.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. Объекты и признаки\n",
    "\n",
    "Признаком $f$ объекта $x$  называется отображение $f: X \\rightarrow D_f$, где $D_f$ - множество допустимых значений признака. То есть признак - результат измерения некоторой характеристики объекта.\n",
    "\n",
    "В зависимости от природы множества $D_f$ признаки делятся на несколько типов:\n",
    "- Если $D_f = \\{0, 1\\}$, то $f$ бинарный признак\n",
    "- Если $D_f$ это конечное множество, то $f$ - номинальный признак\n",
    "- Если $D_f$ это конечное упорядоченное множество, то $f$ - порядковый признак\n",
    "- Если $D_f = R$, то $f$ - количественный признак\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3. Основные типы задач обучения с учителем\n",
    "\n",
    "В зависимости от природы множества $Y$ задачи обучения с учителем делятся на следующие типы:\n",
    "\n",
    "- Если множество $Y = \\{1, 2, ..., M\\}$, то задача называется задачей классификации на $M$ непересекающихся классов\n",
    "- Если $Y = \\{0, 1\\}^M$ - задача классификации на $M$ пересекающихся классов\n",
    "- Если $Y = R$ - задача регрессии\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4. Функция потерь и функционал качества\n",
    "\n",
    "Функция потерь, она же loss function, это неотрицательная функция $L(a, x)$, характеризующая величину ошибки решающей функции $a$, приближающей целевую функцию $y^*$ на объекте $x$. Если $L(a, x) = 0$, то ответ $a(x)$ называется корректным.\n",
    "\n",
    "Функционалом качества или эмпирическим риском называется среднее арифметическое функции потерь на выборке $X^l$: $$Q(a, X^l) = \\frac{1}{l}\\sum_{i=0}^{i=l}{L(a, x_i)}$$\n",
    "\n",
    "Истинным риском назвается математическое ожидание функции потерь на всем множесве $X$.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Понятие модели\n",
    "\n",
    "Моделью называется параметрическое семейство отображений $A = \\{a(x, \\theta) | \\theta \\in \\Theta\\}$, где $a: X × \\Theta \\rightarrow Y$ некоторая фиксированная функция, $\\Theta$ множество допустимых значений параметра $\\theta$, называемое пространством параметров или пространством поиска.\n",
    "\n",
    "Неформально это можно понимать так:\n",
    "\n",
    "Модель это множество функций одинакового типа, но отличающихся каким-то параметром. Например множество линейных функций вида $y = ax + b$. Параметрами здесь являются $a$ и $b$, а моделью все возможные линейные функции. То есть можно можно записать данную модель так:\n",
    "$A = \\{a(x, (a, b)) | a \\in R, b \\in R \\}$. $\\theta$ - пара чисел $(a, b)$, $\\Theta$ - множество всех возможных значений пары чисел $(a, b)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6 Понятие обучения\n",
    "\n",
    "Обучением модели называется процесс подбора параметра $\\theta$ функции $a$, принадлежащей модели $A$, по выборке $X^l$. \n",
    "\n",
    "Методом обучения называется отображение $\\mu = (X × Y)^l \\rightarrow A$, которое произвольной конечной выборке $X^l$ ставит в соответствие функцию $a \\in A$.\n",
    "\n",
    "Классический метод обучения, называемый минимизацией эмпирического риска, empirical risk minimization ERM, заключается в том, чтобы найти в заданной модели $A$ функцию $a$, доставляющую минимальное значение функционалу качества на заданной обучающей выборке $X^l$: $$\\hat\\theta = argmin_{a \\in A}Q(a, X^l)$$\n",
    "\n",
    "Неформально говоря:\n",
    "\n",
    "Мы предполагаем, что целевая функция $y^*$ принадлежит некоторому классу функций (модели), но конкретные параметры этой функции нам неизвестны. Обучение — это процесс выбора параметров модели по выборке так, чтобы функция с этими параметрами минимизировала ошибку на данных. Полученная функция и становится решающей функцией $a$.\n",
    "\n",
    "Например, мы предполагаем что таргет это линейная функция, то есть принадлежит линейной модели и имеет вид $y^*= ax+b$. Обучением будет подбор параметром $(a, b)$ по выборке так, чтобы минимизировать ошибку.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.7 Проблемы обучения - underfitting и overfitting\n",
    "\n",
    "Зачастую при обучении модели возникают две основные проблемы:\n",
    "\n",
    "- Недообучиение (underfitting) - модель слишком простая и не способна уловить зависимость в данных.\n",
    "В результате ошибки как на обучающей выборке, так и на новых данных остаются большими.\n",
    "- Переобучение (overfitting) — модель слишком сложная и запоминает все детали обучающей выборки, включая шум.\n",
    "В результате ошибка на обучающих данных низкая, но на новых данных значительно выше.\n",
    "\n",
    "Хорошая модель это та, которая улавливает закономерности и не запоминает шум.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.8 Проблемы обучения - bias-variance tradeoff\n",
    "\n",
    "Для того чтобы модель не переобучалась, но и не недообучалась необходимо найти баланс в сложности модели. Такой баланс называют bias-variance tradeoff. \n",
    "\n",
    "- bias (смещение) - систематическая ошибка модели\n",
    "- variance (разброс) - чувствительность модели к шуму в данных \n",
    "\n",
    "Если модель слишком простая, у неё высокий bias и низкий variance.\n",
    "Если модель слишком сложная, у неё низкий bias, но высокий variance.\n",
    "\n",
    "Пример: пусть целевая функция квадратичная.\n",
    "\n",
    "- Линейная модель будет слишком простой: её предсказания систематически ошибочны (высокий bias), но она меньше реагирует на шум (низкий variance).\n",
    "- Модель с кубической функцией будет слишком гибкой: она может запоминать шум и детали обучающей выборки (высокий variance), хотя bias будет низким.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.9 Регуляризация\n",
    "\n",
    "Регуляризация - это способ ограничить сложность модели, чтобы уменьшить переобучение. В классическом ERM добавляется штраф за слишком большие или слишком сложные параметры модели:\n",
    "$$\\hat\\theta = argmin_{a \\in A}[Q(a, X^l) + \\lambda\\Omega(\\theta)]$$\n",
    "- $Q(a, X^l)$ - эмпирический риск\n",
    "- $\\Omega$ - функция регуляризации\n",
    "- $\\lambda$ - коэффициент регуляризации, задающий степень ограничения\n",
    "\n",
    "Самые популярные виды регуляризации:\n",
    "- L2 (Ridge): уменьшает все веса, стабилизирует модель\n",
    "- L1 (Lasso): выполняет отбор признаков, обнуляя некоторые веса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300a347",
   "metadata": {},
   "source": [
    "## 2. Задача регрессия\n",
    "\n",
    "Регрессия - это задача обучения с учителем, где целевые значения $Y \\in R$.\n",
    "Цель - построить функцию $a(x)$ которая приближает значения целевой функции $y^∗$на новых данных.\n",
    "\n",
    "Например: предсказание цены, температуры, дохода.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Линейная регрессия\n",
    "\n",
    "### 2.1.1 Модель\n",
    "\n",
    "Линейная регрессия это модель вида: $$f(x) = w^T x + b$$\n",
    "Где:\n",
    "- $x$ - вектор признаков\n",
    "- $w$ — вектор весов\n",
    "- $b$ — смещение\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2 Функции потерь\n",
    "\n",
    "#### Среднеквадратичная ошибка (MSE, Mean Squared Error)\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{l} \\sum_{i=1}^{l} (y_i - a(x_i))^2$$\n",
    "\n",
    "Особенности: сильно штрафует большие выбросы\n",
    "\n",
    "#### Корень из среднеквадратичной ошибки (RMSE, Root Mean Squared Error)\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{l} \\sum_{i=1}^{l} (y_i - a(x_i))^2} = \\sqrt{\\text{MSE}}$$\n",
    "\n",
    "Особенности: удобно интерпретировать, так как единицы совпадают с исходными данными.  \n",
    "\n",
    "#### Средняя абсолютная ошибка (MAE, Mean Absolute Error)\n",
    "$$\\text{MAE} = \\frac{1}{l} \\sum_{i=1}^{l} |y_i - a(x_i)|$$\n",
    "Особенности: менее чувствительна к выбросам, чем MSE. \n",
    "\n",
    "#### MAPE (Mean Absolute Percentage Error)\n",
    "$$\\text{MAPE} = \\frac{100\\%}{l} \\sum_{i=1}^{l} \\frac{|y_i - a(x_i)|}{|y_i|}, y_i \\neq 0$$\n",
    "Особенности: полезна для бизнес-прогнозов, но не работает, если $y_i = 0$.  \n",
    "\n",
    "#### Коэффициент детерминации\n",
    "Коэффициент, показывающий долю вариации целевой переменной, объясненную моделью.  \n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{l} (y_i - a(x_i))^2}{\\sum_{i=1}^{l} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "где $\\bar{y}$ — среднее по выборке.\n",
    "Особенности: \n",
    "- $R^2 = 1$ — идеальное предсказание  \n",
    "- $R^2 = 0$ — модель не лучше среднего  \n",
    "- $R^2 < 0$ — модель хуже простого среднего \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.3 Обучение\n",
    " \n",
    "Обучение линейной регрессии сводится к подбору параметров $w$ и $b$, минимизирующих функцию потерь (обычно MSE) на обучающей выборке:\n",
    "\n",
    "$$\n",
    "\\hat{w}, \\hat{b} = \\arg\\min_{w, b} \\frac{1}{l} \\sum_{i=1}^{l} (y_i - (w^T x_i + b))^2\n",
    "$$\n",
    "\n",
    "Методы обучения:\n",
    "- Аналитическое решение (метод нормальных уравнений): подходит для небольшого числа признаков, вычисляет точное решение напрямую.  \n",
    "- Градиентный спуск: подходит для больших наборов данных и признаков, обновляет параметры итерационно, двигаясь по направлению уменьшения ошибки.  \n",
    "- Стохастический/мини-батч градиентный спуск: вариант градиентного спуска, который ускоряет обучение на больших данных.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.4 Регуляризация \n",
    "\n",
    "- Ridge (L2-регуляризация):\n",
    "$$\n",
    "\\hat{w}, \\hat{b} = \\arg\\min_{w, b} \\frac{1}{l} \\sum_{i=1}^{l} (y_i - (w^T x_i + b))^2 + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "- Lasso (L1-регуляризация):\n",
    "$$\n",
    "\\hat{w}, \\hat{b} = \\arg\\min_{w, b} \\frac{1}{l} \\sum_{i=1}^{l} (y_i - (w^T x_i + b))^2 + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "$$\n",
    "\n",
    "Интуитивно: \n",
    "- L2 (Ridge) слегка уменьшает все веса, делая модель более «гладкой» и устойчивой к шуму.  \n",
    "- L1 (Lasso) может обнулять некоторые веса, фактически отбирая только важные признаки, что упрощает модель.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
